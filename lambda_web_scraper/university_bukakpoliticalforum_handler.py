import re
from datetime import datetime, timedelta
import pytz
from typing import Dict, Any
from common_utils import (
    fetch_page,
    get_recent_notices,
    save_notices_to_db,
    send_slack_notification,
)


def handler(event, context):
    """
    ë¶ì•…ì •ì¹˜í¬ëŸ¼ ìŠ¤í¬ë˜í¼ Lambda Handler
    ê¹”ë”í•˜ê³  ë…ë¦½ì ì¸ êµ¬í˜„
    """

    print("ğŸš€ [HANDLER] Lambda Handler ì‹œì‘")

    try:
        # ë™ê¸° ìŠ¤í¬ë˜í¼ ì‹¤í–‰
        result = scrape_university_bukakpoliticalforum()

        return {
            "statusCode": 200,
        }

    except Exception as e:
        error_msg = f"Lambda Handler ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ [HANDLER] {error_msg}")
        send_slack_notification(error_msg, "university_bukakpoliticalforum")
        return {
            "statusCode": 500,
        }


def scrape_university_bukakpoliticalforum() -> Dict[str, Any]:
    """
    ë¶ì•…ì •ì¹˜í¬ëŸ¼ì„ ìŠ¤í¬ë˜í•‘í•˜ê³  ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì„ ì²˜ë¦¬
    """

    url = "https://www.kookmin.ac.kr/user/kmuNews/specBbs/bugAgForum/index.do"
    kst = pytz.timezone("Asia/Seoul")

    print(f"ğŸŒ [SCRAPER] ìŠ¤í¬ë˜í•‘ ì‹œì‘ - URL: {url}")

    try:
        # ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        soup = fetch_page(url)

        # ê³µì§€ì‚¬í•­ ëª©ë¡ ìš”ì†Œë“¤ ê°€ì ¸ì˜¤ê¸°
        elements = soup.select(".board_list ul li")
        print(f"ğŸ“Š [SCRAPER] ë°œê²¬ëœ ê³µì§€ì‚¬í•­ ìˆ˜: {len(elements)}")

        # ê¸°ì¡´ ê³µì§€ì‚¬í•­ í™•ì¸ (MongoDBì—ì„œ)
        recent_notices = get_recent_notices("university_bukakpoliticalforum")
        recent_links = {notice.get("link") for notice in recent_notices}
        recent_titles = {notice.get("title") for notice in recent_notices}

        # ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ íŒŒì‹±
        new_notices = []

        for element in elements:
            notice = parse_notice_from_element(element, kst)
            if notice:
                # 30ì¼ ì´ë‚´ì˜ ë°ì´í„°ë§Œ í•„í„°ë§
                thirty_days_ago = datetime.now(kst) - timedelta(days=30)
                published_date = datetime.fromisoformat(
                    notice["published"].replace("Z", "+00:00")
                )
                if published_date >= thirty_days_ago:
                    # ì¤‘ë³µ í™•ì¸
                    if (
                        notice["link"] not in recent_links
                        and notice["title"] not in recent_titles
                    ):
                        new_notices.append(notice)
                        print(
                            f"ğŸ†• [SCRAPER] ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­: {notice['title'][:30]}..."
                        )
                else:
                    print(
                        f"â° [SCRAPER] 30ì¼ ì´ì „ ê³µì§€ì‚¬í•­ ì œì™¸: {notice['title'][:30]}..."
                    )

        print(f"ğŸ“ˆ [SCRAPER] ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ ìˆ˜: {len(new_notices)}")

        # ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì„ MongoDBì— ì €ì¥
        saved_count = 0
        if new_notices:
            saved_count = save_notices_to_db(
                new_notices, "university_bukakpoliticalforum"
            )
            print(f"ğŸ’¾ [SCRAPER] ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")

        result = {
            "success": True,
            "message": f"ë¶ì•…ì •ì¹˜í¬ëŸ¼ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ",
            "total_found": len(elements),
            "new_notices_count": len(new_notices),
            "saved_count": saved_count,
            "new_notices": new_notices,
        }

        print(f"ğŸ‰ [SCRAPER] ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
        return result

    except Exception as e:
        error_msg = f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ [SCRAPER] {error_msg}")
        send_slack_notification(error_msg, "university_bukakpoliticalforum")
        return {"success": False, "error": error_msg}


def parse_notice_from_element(element, kst) -> Dict[str, Any]:
    """HTML ìš”ì†Œì—ì„œ ë¶ì•…ì •ì¹˜í¬ëŸ¼ ì •ë³´ë¥¼ ì¶”ì¶œ"""

    try:
        # a íƒœê·¸ ì¶”ì¶œ
        a_tag = element.select_one("a")
        if not a_tag:
            print("âŒ [PARSE] a íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ì œëª© ì¶”ì¶œ
        title_tag = a_tag.select_one("p.title")
        if not title_tag:
            print("âŒ [PARSE] ì œëª© ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        title = title_tag.text.strip()

        # ì‘ì„±ì ì¶”ì¶œ
        author_element = a_tag.select_one("p.desc")
        author = author_element.text.strip() if author_element else ""
        if author:
            title = f"[{author}] {title}"

        # íšŒì°¨ ì •ë³´ ì¶”ì¶œ í›„ ì œëª©ì— ì¶”ê°€
        category_element = a_tag.select_one(".ctg_name em")
        category = category_element.text.strip() if category_element else ""
        if category:
            title = f"[{category}] {title}"

        # ë‚ ì§œì™€ ì¥ì†Œ ì¶”ì¶œ
        board_etc = a_tag.select_one(".board_etc")
        if board_etc:
            spans = board_etc.select("span")
            if spans and len(spans) > 0:
                # ë‚ ì§œ ì¶”ì¶œ ë° ë³€í™˜
                date_text = spans[0].text.strip().replace("ì¼ì‹œ ë° ê¸°ê°„: ", "")
                try:
                    # "2025.04.29 (18:45~20:15)" ë˜ëŠ” "2025.04.29 (18:45 ~ 20:15)" í˜•ì‹ì—ì„œ ë‚ ì§œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    date_match = re.search(r"(\d{4})\.(\d{2})\.(\d{2})", date_text)
                    if date_match:
                        year, month, day = date_match.groups()
                        published = datetime.strptime(
                            f"{year}-{month}-{day}", "%Y-%m-%d"
                        ).replace(tzinfo=kst)
                    else:
                        print(f"âŒ [PARSE] ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨: {date_text}")
                        published = datetime.now(kst)
                except ValueError as e:
                    print(f"âŒ [PARSE] ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    published = datetime.now(kst)

                # ì¥ì†Œ ì •ë³´ ì¶”ì¶œ (ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ ë ˆê±°ì‹œ ì½”ë“œì™€ í˜¸í™˜ì„± ìœ ì§€)
                location = spans[1].text.strip() if len(spans) > 1 else ""
            else:
                published = datetime.now(kst)
                location = ""
        else:
            published = datetime.now(kst)
            location = ""

        # ë§í¬ ì¶”ì¶œ
        onclick = a_tag.get("onclick", "")
        if "global.write(" in onclick:
            # onclick="global.write('58873', './view.do');" ì—ì„œ post_id ì¶”ì¶œ
            parts = onclick.split("'")
            if len(parts) >= 2:
                post_id = parts[1]
                link = f"https://www.kookmin.ac.kr/user/kmuNews/specBbs/bugAgForum/view.do?dataSeq={post_id}"
            else:
                link = (
                    "https://www.kookmin.ac.kr/user/kmuNews/specBbs/bugAgForum/index.do"
                )
        else:
            link = "https://www.kookmin.ac.kr/user/kmuNews/specBbs/bugAgForum/index.do"

        # ë¡œê¹…
        print(f"ğŸ“ [PARSE] ê³µì§€ì‚¬í•­ íŒŒì‹±: {title[:50]}...")

        result = {
            "title": title,
            "link": link,
            "published": published.isoformat(),
            "scraper_type": "university_bukakpoliticalforum",
        }

        return result

    except Exception as e:
        print(f"âŒ [PARSE] ê³µì§€ì‚¬í•­ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return None
