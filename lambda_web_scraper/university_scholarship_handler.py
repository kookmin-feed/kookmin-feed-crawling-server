import json
import asyncio
from datetime import datetime
import pytz
from typing import Dict, Any
from common_utils import fetch_page, get_recent_notices, save_notices_to_db


def handler(event, context):
    """
    ëŒ€í•™ ì¥í•™ê³µì§€ ìŠ¤í¬ë˜í¼ Lambda Handler
    ê¹”ë”í•˜ê³  ë…ë¦½ì ì¸ êµ¬í˜„
    """

    print("ğŸš€ [HANDLER] Lambda Handler ì‹œì‘")

    try:
        # ë¹„ë™ê¸° ìŠ¤í¬ë˜í¼ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            result = loop.run_until_complete(scrape_university_scholarship())
        finally:
            loop.close()

        return {
            "statusCode": 200,
            "body": json.dumps(result, ensure_ascii=False, default=str),
        }

    except Exception as e:
        print(f"âŒ [HANDLER] ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {
            "statusCode": 500,
            "body": json.dumps(
                {"error": f"Lambda Handler ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"}, ensure_ascii=False
            ),
        }


async def scrape_university_scholarship() -> Dict[str, Any]:
    """
    ëŒ€í•™ ì¥í•™ê³µì§€ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ê³  ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì„ ì²˜ë¦¬
    """

    url = "https://cs.kookmin.ac.kr/news/kookmin/scholarship/"
    kst = pytz.timezone("Asia/Seoul")

    print(f"ğŸŒ [SCRAPER] ìŠ¤í¬ë˜í•‘ ì‹œì‘ - URL: {url}")

    try:
        # ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        soup = await fetch_page(url)
        if not soup:
            print("âŒ [SCRAPER] ì›¹í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
            return {"success": False, "error": "ì›¹í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

        # ê³µì§€ì‚¬í•­ ëª©ë¡ ìš”ì†Œë“¤ ê°€ì ¸ì˜¤ê¸°
        elements = soup.select(".list-tbody ul")
        print(f"ğŸ“Š [SCRAPER] ë°œê²¬ëœ ê³µì§€ì‚¬í•­ ìˆ˜: {len(elements)}")

        # ê¸°ì¡´ ê³µì§€ì‚¬í•­ í™•ì¸ (MongoDBì—ì„œ)
        recent_notices = get_recent_notices("university_scholarship")
        recent_links = {notice.get("link") for notice in recent_notices}
        recent_titles = {notice.get("title") for notice in recent_notices}

        # ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ íŒŒì‹±
        new_notices = []

        for element in elements:
            notice = parse_notice_from_element(element, kst)
            if notice:
                # ì¤‘ë³µ í™•ì¸
                if (
                    notice["link"] not in recent_links
                    and notice["title"] not in recent_titles
                ):
                    new_notices.append(notice)
                    print(f"ğŸ†• [SCRAPER] ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­: {notice['title'][:30]}...")

        print(f"ğŸ“ˆ [SCRAPER] ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ ìˆ˜: {len(new_notices)}")

        # ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì„ MongoDBì— ì €ì¥
        saved_count = 0
        if new_notices:
            saved_count = save_notices_to_db(new_notices, "university_scholarship")
            print(f"ğŸ’¾ [SCRAPER] ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")

        result = {
            "success": True,
            "message": f"ëŒ€í•™ ì¥í•™ê³µì§€ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ",
            "total_found": len(elements),
            "new_notices_count": len(new_notices),
            "saved_count": saved_count,
            "new_notices": new_notices,
        }

        print(f"ğŸ‰ [SCRAPER] ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
        return result

    except Exception as e:
        print(f"âŒ [SCRAPER] ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"success": False, "error": f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}"}


def parse_notice_from_element(element, kst) -> Dict[str, Any]:
    """HTML ìš”ì†Œì—ì„œ ì¥í•™ê³µì§€ ì •ë³´ë¥¼ ì¶”ì¶œ"""

    try:
        # ê³µì§€ì‚¬í•­ ì—¬ë¶€ í™•ì¸
        is_notice = element.select_one(".notice") is not None

        # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
        title_element = element.select_one(".subject a")
        if not title_element:
            return None

        title = title_element.get_text(strip=True)
        relative_link = title_element.get("href", "")
        link = f"https://cs.kookmin.ac.kr/news/kookmin/scholarship/{relative_link}"

        # ë‚ ì§œ ì¶”ì¶œ
        date_element = element.select_one(".date")
        if not date_element:
            return None

        date_str = date_element.get_text(strip=True)

        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
        try:
            # YYYY-MM-DD í˜•ì‹
            published = datetime.strptime(date_str, "%Y-%m-%d").replace(tzinfo=kst)
        except ValueError:
            try:
                # YYYY.MM.DD í˜•ì‹
                published = datetime.strptime(date_str, "%Y.%m.%d").replace(tzinfo=kst)
            except ValueError:
                # YY.MM.DD í˜•ì‹
                published = datetime.strptime(date_str, "%y.%m.%d").replace(tzinfo=kst)

        result = {
            "title": title,
            "link": link,
            "published": published,
            "scraper_type": "university_scholarship",
            "korean_name": "ëŒ€í•™ ì¥í•™ê³µì§€",
        }

        return result

    except Exception as e:
        print(f"âŒ [PARSE] ê³µì§€ì‚¬í•­ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return None
