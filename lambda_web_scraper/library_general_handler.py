import re
from datetime import datetime, timedelta
from typing import Dict, Any
import pytz

from common_utils import get_recent_notices, save_notices_to_db, send_slack_notification


def parse_date(date_str: str, kst: pytz.timezone) -> datetime:
    """
    ë‚ ì§œ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ datetime ê°ì²´ë¡œ ë³€í™˜
    """
    try:
        # "9ì›” 4ì¼" í˜•ì‹ íŒŒì‹±
        if "ì›”" in date_str and "ì¼" in date_str:
            month_day_match = re.search(r"(\d{1,2})ì›”\s*(\d{1,2})ì¼", date_str)
            if month_day_match:
                month = int(month_day_match.group(1))
                day = int(month_day_match.group(2))
                current_year = datetime.now(kst).year
                return kst.localize(datetime(current_year, month, day))

        # ê¸°ë³¸ê°’: í˜„ì¬ ì‹œê°„
        return datetime.now(kst)
    except Exception as e:
        print(f"âš ï¸ [PARSER] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str}, ì˜¤ë¥˜: {e}")
        return datetime.now(kst)


def _setup_browser():
    """ë¸Œë¼ìš°ì € ì„¤ì • ë° í˜ì´ì§€ ìƒì„±"""
    from playwright.sync_api import sync_playwright

    p = sync_playwright().start()
    browser = p.chromium.launch(
        headless=True,
        args=[
            "--no-sandbox",
            "--disable-setuid-sandbox",
            "--disable-dev-shm-usage",
            "--disable-accelerated-2d-canvas",
            "--no-first-run",
            "--no-zygote",
            "--single-process",
            "--disable-gpu",
            "--disable-background-timer-throttling",
            "--disable-backgrounding-occluded-windows",
            "--disable-renderer-backgrounding",
            "--disable-web-security",
            "--disable-features=TranslateUI",
            "--disable-extensions",
        ],
    )

    page = browser.new_page()
    page.set_extra_http_headers(
        {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
    )

    return p, browser, page


def _navigate_to_main_page(page, url):
    """ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™ ë° í…Œì´ë¸” ë¡œë”© ëŒ€ê¸°"""
    page.goto(url, timeout=30000)
    page.wait_for_selector("table.ikc-bulletins", timeout=10000)
    page.wait_for_timeout(2000)


def _get_notice_rows(page):
    """ê³µì§€ì‚¬í•­ í–‰ë“¤ì„ ê°€ì ¸ì˜¤ê¸° (ìµœì‹  10ê°œë§Œ)"""
    rows = page.query_selector_all("table.ikc-bulletins tbody tr.ng-star-inserted")
    rows = rows[:10]  # ìµœì‹ ìˆœìœ¼ë¡œ 10ê°œë§Œ ì²˜ë¦¬
    print(f"ğŸ“Š [SCRAPER] ë°œê²¬ëœ ê³µì§€ì‚¬í•­ ìˆ˜: {len(rows)} (ìµœì‹  10ê°œë§Œ ì²˜ë¦¬)")
    return rows


def _extract_title_from_row(row, index):
    """í–‰ì—ì„œ ì œëª© ì¶”ì¶œ"""
    title_element = row.query_selector("td:nth-child(2) span")
    if not title_element:
        print(f"âš ï¸ [SCRAPER] ê³µì§€ì‚¬í•­ {index+1} ì œëª© ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None

    title = title_element.evaluate("el => el.textContent").strip()
    print(f"ğŸ“° [SCRAPER] ì œëª©: '{title}'")
    return title, title_element


def _extract_date_from_detail_page(page, kst):
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ"""
    page.wait_for_timeout(2000)

    date_text = ""
    properties_elements = page.query_selector_all("ul.ikc-bulletin-properties li span")

    if len(properties_elements) >= 2:
        date_text = properties_elements[1].text_content().strip()

        # ì‹œê°„ í˜•ì‹ì¸ì§€ í™•ì¸ ("ì˜¤ì „ 9:14", "ì˜¤í›„ 2:30" ë“±)
        if re.match(r"(ì˜¤ì „|ì˜¤í›„)\s+\d{1,2}:\d{2}", date_text):
            date_text = datetime.now(kst).strftime("%mì›” %dì¼")
        elif not re.match(r"\d{1,2}ì›”\s+\d{1,2}ì¼", date_text):
            date_text = datetime.now(kst).strftime("%mì›” %dì¼")
    else:
        date_text = datetime.now(kst).strftime("%mì›” %dì¼")

    return parse_date(date_text, kst)


def _create_notice_data(title, link, published_date):
    """ê³µì§€ì‚¬í•­ ë°ì´í„° ìƒì„±"""
    return {
        "title": title,
        "link": link,
        "published": published_date.isoformat(),
        "scraper_type": "library_general",
    }


def _return_to_main_page(page, url):
    """ë©”ì¸ í˜ì´ì§€ë¡œ ëŒì•„ê°€ê¸°"""
    page.goto(url, timeout=30000)
    page.wait_for_selector("table.ikc-bulletins", timeout=10000)
    page.wait_for_timeout(1000)


def _process_single_notice(page, row, index, url, recent_titles, recent_links, kst):
    """ë‹¨ì¼ ê³µì§€ì‚¬í•­ ì²˜ë¦¬"""
    try:
        print(f"ğŸ” [SCRAPER] ê³µì§€ì‚¬í•­ {index+1} ì²˜ë¦¬ ì‹œì‘")

        # ì œëª© ì¶”ì¶œ
        title_info = _extract_title_from_row(row, index)
        if not title_info:
            return None

        title, title_element = title_info

        # ê¸°ì¡´ ê³µì§€ì‚¬í•­ í™•ì¸
        if title in recent_titles:
            print(f"â™»ï¸ [SCRAPER] ê¸°ì¡´ ê³µì§€ì‚¬í•­ (ìŠ¤í‚µ): {title[:30]}...")
            return None

        # ì œëª© í´ë¦­í•˜ì—¬ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
        title_element.click()
        page.wait_for_timeout(3000)
        actual_link = page.url

        # ë§í¬ ì¤‘ë³µ í™•ì¸
        if actual_link in recent_links:
            print(f"â™»ï¸ [SCRAPER] ê¸°ì¡´ ë§í¬ (ìŠ¤í‚µ): {actual_link}")
            _return_to_main_page(page, url)
            return None

        # ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        try:
            published = _extract_date_from_detail_page(page, kst)
            notice_data = _create_notice_data(title, actual_link, published)

            # 30ì¼ ì´ë‚´ í•„í„°ë§
            thirty_days_ago = datetime.now(kst) - timedelta(days=30)
            published_date = datetime.fromisoformat(
                notice_data["published"].replace("Z", "+00:00")
            )

            if published_date >= thirty_days_ago:
                print(f"ğŸ†• [SCRAPER] ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ ì¶”ê°€: {title[:30]}...")
                return notice_data
            else:
                print(f"â° [SCRAPER] 30ì¼ ì´ì „ ê³µì§€ì‚¬í•­ ì œì™¸: {title[:30]}...")
                return None

        except Exception as e:
            print(f"âš ï¸ [SCRAPER] ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            notice_data = _create_notice_data(title, actual_link, datetime.now(kst))
            print(f"ğŸ†• [SCRAPER] ê¸°ë³¸ ë°ì´í„°ë¡œ ê³µì§€ì‚¬í•­ ì¶”ê°€: {title[:30]}...")
            return notice_data

    except Exception as e:
        print(f"âš ï¸ [SCRAPER] ê³µì§€ì‚¬í•­ {index+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    finally:
        _return_to_main_page(page, url)


def _save_notices_to_db(new_notices):
    """ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ë“¤ì„ DBì— ì €ì¥"""
    saved_count = 0
    if new_notices:
        print(f"ğŸ’¾ [DB] ì €ì¥ ì‹œë„í•  ê³µì§€ì‚¬í•­ ìˆ˜: {len(new_notices)}")
        saved_count = save_notices_to_db(new_notices, "library_general")
        print(f"ğŸ’¾ [DB] ì‹¤ì œ ì €ì¥ëœ ê³µì§€ì‚¬í•­ ìˆ˜: {saved_count}")

        if saved_count != len(new_notices):
            print(f"âš ï¸ [DB] ì €ì¥ ì‹¤íŒ¨: ì‹œë„ {len(new_notices)}ê°œ, ì„±ê³µ {saved_count}ê°œ")
    else:
        print("â„¹ï¸ [DB] ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ì–´ ì €ì¥í•˜ì§€ ì•ŠìŒ")

    return saved_count


def scrape_library_general() -> Dict[str, Any]:
    """
    ì„±ê³¡ë„ì„œê´€ ì¼ë°˜ê³µì§€ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜
    í´ë¦­ ê¸°ë°˜ ìŠ¤í¬ë˜í•‘ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë§í¬ì™€ ë‚´ìš©ì„ ì¶”ì¶œ
    """
    url = "https://lib.kookmin.ac.kr/library-guide/notice"
    kst = pytz.timezone("Asia/Seoul")

    print(f"ğŸŒ [SCRAPER] ìŠ¤í¬ë˜í•‘ ì‹œì‘ - URL: {url}")

    try:
        p, browser, page = _setup_browser()
        new_notices = []

        try:
            _navigate_to_main_page(page, url)
            rows = _get_notice_rows(page)

            if not rows:
                print("âŒ [SCRAPER] ê³µì§€ì‚¬í•­ í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return {
                    "statusCode": 500,
                    "body": {"message": "ê³µì§€ì‚¬í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"},
                }

            # ê¸°ì¡´ ê³µì§€ì‚¬í•­ í™•ì¸
            recent_notices = get_recent_notices("library_general")
            recent_links = {notice.get("link") for notice in recent_notices}
            recent_titles = {notice.get("title") for notice in recent_notices}
            print(f"ğŸ“‹ [DB] ê¸°ì¡´ ê³µì§€ì‚¬í•­ ìˆ˜: {len(recent_notices)}")

            # ê° ê³µì§€ì‚¬í•­ ì²˜ë¦¬
            for i in range(len(rows)):
                # ë§¤ë²ˆ ìƒˆë¡œìš´ ìš”ì†Œ ì¿¼ë¦¬ (ElementHandle ë¬´íš¨í™” ë¬¸ì œ í•´ê²°)
                current_rows = page.query_selector_all(
                    "table.ikc-bulletins tbody tr.ng-star-inserted"
                )
                if i >= len(current_rows):
                    print(f"âš ï¸ [SCRAPER] ê³µì§€ì‚¬í•­ {i+1} ì¸ë±ìŠ¤ ì´ˆê³¼")
                    continue

                row = current_rows[i]
                notice_data = _process_single_notice(
                    page, row, i, url, recent_titles, recent_links, kst
                )

                if notice_data:
                    new_notices.append(notice_data)

        finally:
            browser.close()
            p.stop()

        print(f"ğŸ“ˆ [SCRAPER] ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ ìˆ˜: {len(new_notices)}")
        saved_count = _save_notices_to_db(new_notices)

        return {
            "statusCode": 200,
            "body": {
                "message": "ìŠ¤í¬ë˜í•‘ ì™„ë£Œ",
                "scraped_count": len(rows),
                "new_count": len(new_notices),
                "saved_count": saved_count,
            },
        }

    except Exception as e:
        error_msg = f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ [SCRAPER] {error_msg}")
        send_slack_notification(error_msg, "library_general")
        raise e


def handler(event, context):
    """
    AWS Lambda í•¸ë“¤ëŸ¬ í•¨ìˆ˜
    """
    print("ğŸš€ [HANDLER] Lambda Handler ì‹œì‘ - ì„±ê³¡ë„ì„œê´€ ì¼ë°˜ê³µì§€")

    try:
        result = scrape_library_general()
        return result
    except Exception as e:
        error_msg = f"Lambda í•¸ë“¤ëŸ¬ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ [HANDLER] {error_msg}")
        return {
            "statusCode": 500,
            "body": {"message": error_msg},
        }
